#sigmoid
tf.nn.sigmoid(x)\
特点：\
1，易造成梯度消失\
（因为他的导数的取值范围是0到0.25，多个这样连续的导数相乘，
会导致值趋近于0，导致数值无法更新，引发梯度消失）\
2，输出非0均值，收敛慢\
3，幂运算复杂，训练时间长\

#tanh
tf.math.tanh(x)\
特点：\
1，输出是0均值\
2，易造成梯度消失\
3，幂运算复杂，训练时间长\

#ReLU
tf.nn.relu(x)\
优点：\
1，解决了梯度消失问题（在正区间）\
2，只需要判断输入是否大于0，计算速度快\
3，首里安速度远快于sigmoid和tanh\

缺点：\
1，输出非0均值，收敛慢\
2，Dead ReLU问题，某些神经元可能永远不会被激活，导致相应的参数永远不能被更新\
（送入激活函数的特征是负数时，激活函数的输出为0，
反向传播得到的梯度为0，导致参数无法更新。经过ReLU函数的负数特征过多导致的）

#Leaky ReLU
tf.nn.leaky_relu(x)\
理论上来讲，leaky relu有所有ReLU的所有优点，外加不会有dead ReLU问题，
但是在实际操作当中，并没有完全证明L ReLU总是好于RL。

#对于初学者的建议
1，首选ReLU激活函数\
2，学习率设置较小值\
3，输入特征标准化，即让输入特征满足以0为均值，1为标准差的正态分布\
4，初始参数中心化，即让随机生成的参数满足以0为均值，
2除以当前层输入特征个数的开平方为标准差的正态分布。